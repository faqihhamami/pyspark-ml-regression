{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark23/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('decisiontree-regression') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "        .options(header='true', inferSchema='true') \\\n",
    "        .load('/home/hadoop/project/pydone/spark/dataset/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|               200|              200|               200|               200|               200|\n",
      "|   mean|             100.5|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|57.879184513951124|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|                 1|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|               200|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert data to dense vector\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##convert data to dense vector\n",
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "|[7.0,57.5,32.8,23.5]| 11.8|\n",
      "|[8.0,120.2,19.6,1...| 13.2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "| [6.0,8.7,48.9,75.0]|  7.2|\n",
      "|   [9.0,8.6,2.1,1.0]|  4.8|\n",
      "|[11.0,66.1,5.8,24.2]|  8.6|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decision tree library\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dc = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "model = dc.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'label', 'prediction']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show prediction columns\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|              9.45|\n",
      "|[4.0,151.5,41.3,5...| 18.5|            20.125|\n",
      "| [6.0,8.7,48.9,75.0]|  7.2|               5.5|\n",
      "|   [9.0,8.6,2.1,1.0]|  4.8|               5.5|\n",
      "|[11.0,66.1,5.8,24.2]|  8.6|               9.4|\n",
      "|[13.0,23.8,35.1,6...|  9.2| 8.624999999999998|\n",
      "| [14.0,97.5,7.6,7.2]|  9.7|               9.4|\n",
      "|[19.0,69.2,20.5,1...| 11.3|12.036363636363633|\n",
      "|[22.0,237.4,5.1,2...| 12.5|11.871428571428572|\n",
      "|[23.0,13.2,15.9,4...|  5.6|               5.5|\n",
      "|[25.0,62.3,12.6,1...|  9.7|12.036363636363633|\n",
      "|[26.0,262.9,3.5,1...| 12.0|11.871428571428572|\n",
      "|[28.0,240.1,16.7,...| 15.9|             15.88|\n",
      "|[29.0,248.8,27.1,...| 18.9|             20.85|\n",
      "|[31.0,292.9,28.3,...| 21.4|             20.85|\n",
      "|[32.0,112.9,17.4,...| 11.9|12.036363636363633|\n",
      "|[33.0,97.2,1.5,30.0]|  9.6|               9.4|\n",
      "|[36.0,290.7,4.1,8.5]| 12.8|11.871428571428572|\n",
      "|[43.0,293.6,27.7,...| 20.7|             20.85|\n",
      "|[50.0,66.9,11.7,3...|  9.7|12.036363636363633|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select few rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  1.4588326224649573\n"
     ]
    }
   ],
   "source": [
    "#regresion evaluator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.9039338084569278\n"
     ]
    }
   ],
   "source": [
    "#compare prediction label and datatest label\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
