{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark23/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Gradient-Boosted-Tree') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "        .options(header='true', inferSchema='true') \\\n",
    "        .load('/home/hadoop/project/pydone/spark/dataset/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|               200|              200|               200|               200|               200|\n",
      "|   mean|             100.5|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|57.879184513951124|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|                 1|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|               200|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert data to dense vector\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##convert data to dense vector\n",
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "| [6.0,8.7,48.9,75.0]|  7.2|\n",
      "|   [9.0,8.6,2.1,1.0]|  4.8|\n",
      "|[10.0,199.8,2.6,2...| 10.6|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[7.0,57.5,32.8,23.5]| 11.8|\n",
      "|[8.0,120.2,19.6,1...| 13.2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gbt \n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "model = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'label', 'prediction']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show prediction columns\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[1.0,230.1,37.8,6...| 22.1|21.253329294377615|\n",
      "|[2.0,44.5,39.3,45.1]| 10.4| 8.865646051676359|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|7.2118139550325555|\n",
      "|[7.0,57.5,32.8,23.5]| 11.8| 8.601102731517921|\n",
      "|[8.0,120.2,19.6,1...| 13.2|12.535456859159291|\n",
      "|[12.0,214.7,24.0,...| 17.4|15.830376236508691|\n",
      "| [14.0,97.5,7.6,7.2]|  9.7|10.499749531773967|\n",
      "|[19.0,69.2,20.5,1...| 11.3|10.558131674696384|\n",
      "|[21.0,218.4,27.7,...| 18.0|19.676421455890996|\n",
      "|[22.0,237.4,5.1,2...| 12.5|12.482726185901605|\n",
      "|[25.0,62.3,12.6,1...|  9.7| 8.701136399802087|\n",
      "|[29.0,248.8,27.1,...| 18.9| 19.67306010951847|\n",
      "|[32.0,112.9,17.4,...| 11.9|12.991110608684256|\n",
      "|[33.0,97.2,1.5,30.0]|  9.6|  8.56741708756139|\n",
      "| [35.0,95.7,1.4,7.4]|  9.5| 8.262491844315893|\n",
      "|[38.0,74.7,49.4,4...| 14.7| 12.97589672898006|\n",
      "|[39.0,43.1,26.7,3...| 10.1| 8.654229789173908|\n",
      "|[42.0,177.0,33.4,...| 17.1| 18.92510196708689|\n",
      "|[43.0,293.6,27.7,...| 20.7|20.154332997104607|\n",
      "|[47.0,89.7,9.9,35.7]| 10.6|10.665261449183093|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select few rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  1.1696235306402385\n"
     ]
    }
   ],
   "source": [
    "#regresion evaluator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.9397059684644741\n"
     ]
    }
   ],
   "source": [
    "#compare prediction label and datatest label\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
