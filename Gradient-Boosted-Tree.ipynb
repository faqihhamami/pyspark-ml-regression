{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark23/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Gradient-Boosted-Tree') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "        .options(header='true', inferSchema='true') \\\n",
    "        .load('/home/hadoop/project/pydone/spark/dataset/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|               200|              200|               200|               200|               200|\n",
      "|   mean|             100.5|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|57.879184513951124|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|                 1|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|               200|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert data to dense vector\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##convert data to dense vector\n",
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,230.1,37.8,6...| 22.1|\n",
      "|[3.0,17.2,45.9,69.3]|  9.3|\n",
      "|[4.0,151.5,41.3,5...| 18.5|\n",
      "|[5.0,180.8,10.8,5...| 12.9|\n",
      "| [6.0,8.7,48.9,75.0]|  7.2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|\n",
      "|[7.0,57.5,32.8,23.5]| 11.8|\n",
      "|[8.0,120.2,19.6,1...| 13.2|\n",
      "|[16.0,195.4,47.7,...| 22.4|\n",
      "|[23.0,13.2,15.9,4...|  5.6|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gbt \n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "model = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'label', 'prediction']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show prediction columns\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[2.0,44.5,39.3,45.1]| 10.4|14.379826704342744|\n",
      "|[7.0,57.5,32.8,23.5]| 11.8|12.848286218158684|\n",
      "|[8.0,120.2,19.6,1...| 13.2|13.114437015214182|\n",
      "|[16.0,195.4,47.7,...| 22.4|24.269452680239983|\n",
      "|[23.0,13.2,15.9,4...|  5.6| 5.628785290241136|\n",
      "|[30.0,70.6,16.0,4...| 10.5|10.502066905437204|\n",
      "|[31.0,292.9,28.3,...| 21.4|18.641254954748042|\n",
      "|[33.0,97.2,1.5,30.0]|  9.6|10.713785006679453|\n",
      "|[36.0,290.7,4.1,8.5]| 12.8|  13.0980007953511|\n",
      "|[41.0,202.5,22.3,...| 16.6|15.793327469616942|\n",
      "|[42.0,177.0,33.4,...| 17.1|14.939945349631397|\n",
      "|[43.0,293.6,27.7,...| 20.7|18.145196022142954|\n",
      "|[44.0,206.9,8.4,2...| 12.9|11.787802948974255|\n",
      "|[45.0,25.1,25.7,4...|  8.5| 6.711110391027804|\n",
      "|[46.0,175.1,22.5,...| 14.9|  13.8939849673923|\n",
      "|[49.0,227.2,15.8,...| 14.8|15.185048166354944|\n",
      "|[50.0,66.9,11.7,3...|  9.7|10.267455235128368|\n",
      "|[54.0,182.6,46.2,...| 21.2|17.948563730946255|\n",
      "|[55.0,262.7,28.8,...| 20.2|18.241077629962817|\n",
      "|[60.0,210.7,29.5,...| 18.4|17.285674474396654|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select few rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  1.4904998896228807\n"
     ]
    }
   ],
   "source": [
    "#regresion evaluator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.9201903939689893\n"
     ]
    }
   ],
   "source": [
    "#compare prediction label and datatest label\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
